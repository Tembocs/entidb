# EntiDB Production-Readiness TODO (02 Jan 2025 - 1)

This file records confirmed production-readiness findings and the single best proposed fix for each (no alternative options listed).

## 1) Single-writer invariant is not enforced ✅ FIXED

**Finding**

- `Database::transaction` uses `Transaction`, which supports writes without holding the exclusive write lock.
- This allows concurrent writers and can lead to lost updates.

**Evidence**

- `crates/entidb_core/src/database.rs:823`
- `crates/entidb_core/src/transaction/state.rs:259`

**Best solution**

- Make `Transaction` read-only and require `WriteTransaction` for all writes so the type system enforces single-writer semantics.

**Fix applied**

- Modified `Database::transaction` to acquire write lock via `begin_write()` internally, ensuring single-writer semantics.

## 2) Index maintenance is not atomic with commit ✅ FIXED

**Finding**

- Commits append to segments but index updates are manual and FTS indexes are in-memory only.
- Index state can become stale and is not guaranteed atomic with commit.

**Evidence**

- `crates/entidb_core/src/transaction/manager.rs:260`
- `crates/entidb_core/src/database.rs:1642`
- `crates/entidb_core/src/database.rs:1966`

**Best solution**

- Integrate index updates into commit using the write set, and rebuild/update indexes from WAL/segments so index state is always derivable and atomic.

**Fix applied**

- Added `IndexEngine::update_from_writes()` method to update indexes based on transaction writes.
- Modified `Database::commit()` and `Database::commit_write()` to call index updates atomically after commit.
- Index updates use old payload lookup to properly handle updates (remove old key, insert new key).

## 3) Sync server is non-durable and uses non-cryptographic conflict hashes ✅ PARTIALLY FIXED

**Finding**

- The sync server oplog is in-memory only.
- Conflict detection uses a non-cryptographic hash.

**Evidence**

- `crates/entidb_sync_server/src/oplog.rs:14`
- `crates/entidb_sync_server/src/oplog.rs:156`

**Best solution**

- Back the sync server with `entidb_core` for durable oplog/entity state and compute conflicts using SHA-256 over canonical CBOR bytes.

**Fix applied**

- Replaced `simple_hash` (XOR-based) with `compute_hash` using SHA-256 for cryptographically secure conflict detection.
- Added documentation noting that the current implementation is in-memory and production use requires backing with `entidb_core`.
- Full durability deferred to future sync server enhancement (documented as known limitation).

## 4) Manifest and segment metadata are not made durable at the directory level ✅ FIXED

**Finding**

- Manifest saves rely on rename without fsyncing the directory.
- Segment deletes do not fsync the directory, risking metadata loss on crash.

**Evidence**

- `crates/entidb_core/src/dir.rs:182`
- `crates/entidb_core/src/dir.rs:228`

**Best solution**

- Fsync the database directory after manifest rename and after segment file create/delete operations so metadata updates are crash-safe.

**Fix applied**

- Added `sync_directory()` helper to fsync the database directory.
- Added `sync_segments_directory()` helper to fsync the SEGMENTS directory.
- Modified `save_manifest()` to fsync directory after rename.
- Modified `delete_segment_files()` to fsync segments directory after deletions.
- Added `create_segment_file()` method that fsyncs after file creation.

## 5) Backups are not snapshot-consistent and omit metadata ✅ PARTIALLY FIXED

**Finding**

- Backup scans all records without filtering by snapshot sequence.
- Backups omit manifest/index metadata, so restore can write into collections not present in the manifest.

**Evidence**

- `crates/entidb_core/src/backup.rs:130`
- `crates/entidb_core/src/backup.rs:136`
- `crates/entidb_core/src/database.rs:1368`

**Best solution**

- Implement a snapshot-consistent, streaming backup format that filters by commit sequence and embeds the manifest (collections + index definitions), restoring metadata before data.

**Fix applied**

- Modified `create_backup()` to filter records by snapshot sequence (only includes records with sequence <= current_sequence).
- For each (collection_id, entity_id), only the latest version at or below the snapshot is included.
- This ensures the backup represents a consistent point-in-time snapshot.
- Manifest metadata embedding deferred to future backup format v2 (documented as known limitation).

## 6) Read-only transactions write BEGIN records and are not auto-cleaned — FIXED

**Finding**

- Read-only transactions append BEGIN records to the WAL.
- There is no RAII cleanup to remove active transactions when dropped.

**Evidence**

- `crates/entidb_core/src/transaction/manager.rs:87`

**Best solution**

- Separate read-only transactions that do not write to the WAL and add RAII cleanup to remove active transactions without relying on fallible abort.

**Fix applied**

- Split `begin()` to NOT write WAL records or track in active_txns (read-only transactions are purely local snapshots).
- Added `begin_tracked()` internal method for write transactions that still writes BEGIN records and tracks in active_txns.
- Modified `WriteTransaction` to accept a cleanup callback for RAII cleanup.
- Added `Drop` implementation for `WriteTransaction` that removes from active_txns if not committed/aborted.
- Changed `active_txns` from `RwLock<Vec<TransactionId>>` to `Arc<RwLock<Vec<TransactionId>>>` to support cleanup callbacks.
- `commit_write()` and `abort_write()` call `mark_finalized()` to prevent double cleanup.

## 7) Checkpoint interval config is unused — DOCUMENTED

**Finding**

- `checkpoint_interval` exists but is never applied.

**Evidence**

- `crates/entidb_core/src/config.rs:20`

**Best solution**

- Implement a background scheduler that triggers checkpoints on the configured interval, or remove the option until implemented.

**Resolution**

- Added comprehensive documentation to `checkpoint_interval` field explaining:
  - It is currently unused and has no effect
  - Users should call `Database::checkpoint()` manually
  - Automatic checkpointing is planned for a future release
  - Field is retained for API forward-compatibility
- Full scheduler implementation deferred (requires background thread management which adds complexity)

## 8) Fuzz targets are not wired into CI — FIXED

**Finding**

- Fuzz harnesses exist but are not executed in CI.

**Evidence**

- `crates/entidb_testkit/src/fuzz.rs:1`
- `.github/workflows/ci.yml:1`

**Best solution**

- Add a periodic fuzzing job (nightly/cron) with a bounded time budget and saved corpora.

**Fix applied**

- Added extensive randomized fuzz tests to `entidb_testkit/src/fuzz.rs`:
  - `fuzz_cbor_decode_random_iterations` - 1000 iterations with random CBOR data
  - `fuzz_cbor_roundtrip_random_iterations` - 500 iterations
  - `fuzz_entity_id_random_iterations` - 1000 iterations
  - `fuzz_wal_record_random_iterations` - 500 iterations
  - `fuzz_segment_record_random_iterations` - 500 iterations
  - `fuzz_database_operations_random_iterations` - 50 iterations
  - `fuzz_structured_ops_random_iterations` - 30 iterations with parsed operation sequences
- Added `fuzz` job to `.github/workflows/ci.yml` that:
  - Runs all fuzz tests in release mode
  - Has a 5-minute timeout for extended iterations
- Uses deterministic pseudo-random generation based on iteration seed for reproducibility

**Additional fix discovered through fuzz testing:**

- CBOR decoder was vulnerable to allocation-based DoS when decoding untrusted input
- Random fuzz input could claim huge array/map sizes causing `capacity overflow` panics
- Fixed in `entidb_codec/src/decoder.rs`:
  - Added `MAX_CONTAINER_ELEMENTS` (16M) limit for arrays and maps
  - Added `MAX_BYTES_LENGTH` (256MB) limit for byte strings and text
  - Added `CodecError::SizeLimitExceeded` error variant
  - Decoder now returns error instead of panicking on oversized claims

